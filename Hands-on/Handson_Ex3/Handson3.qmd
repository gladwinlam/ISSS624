---
title: "Hands On Exercise 3"
editor: visual
format: html
execute:
  warning: false
  message: false
  fig-refine: 3
---

# Overview

We can cluster similar regions together using algorithms.

```{r}
pacman::p_load(rgdal, spdep, tmap, sf, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse)
```

## 1. Import Data and Data Preparation

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") #no need CRS. Needed only when dealing with distance stuff. Requires projection.
shan_sf
```

Filter out Shan areas.

```{r}
shan_sf <- shan_sf %>%
   filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)"))
```

```{r}
glimpse(shan_sf)
```

Next we import the aspatial data.

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
summary(ict)

#read_csv() imports as tibble. Field names are preserved. If used read.csv(), imports as base R DF and field names chnage (" " replaced with ".")
```

We might be bias if we look at the absolute number itself. Hence we will turn the absolute value into percentage. We define the penetration rate using the code below.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`)

summary(ict_derived)
```

## 2. Exploratory Data Analysis

We plot distribution of variables using a histogram.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="grey")
```

To detect outliers, we can plot a box plot to help us.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

We plot the new penetration rate.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

For easy comparison, we can plot the penetration rates of the different histograms into. We plot the individual histograms then arrange them together.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")


## Putting all of them together
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)

```

### 2.1 Exploratory Data Analysis using Choropleth Map

We need data from geospatial file and from aspatial file. We join using left_join() and join by sha_sf.TS_PCODE = ict_derived.TS_PCODE

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, 
                     by=c("TS_PCODE"="TS_PCODE"))  #need to be left is geometry data, right attribute. Otherwise lose geometry data if reversed.
```

shan_sf has now both data. We plot a simple map.

```{r}
qtm(shan_sf, "RADIO_PR")
```

We will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map).

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

Regions with more people have higher radio ownership.

We repeat above but plot with radio penetration rate.

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

## 3. Correlation Analysis

```{r}
cluster_vars.cor = cor(ict_derived[,12:17]) #only column 12 to 17
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

## 4. Hierachy Cluster Analysis

First, extract clustering variables. We select "TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR" columns.

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>% #to drop the geometry column becos cannot cluster it
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

We removed internet PR because it is highly correlated with Computer PR. We just need one to reduce multicolinearity problem.

We change index from row number to row name. Then delete TS.x column.

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

Delete TS.x column.

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

### 4.1 Standardisation

Clustering methods need standardisation. Otherwise the clustering will be biased to those with large value range. We use **min max standardisation**. All data fall within 0-1.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

We can do **z score normalisation** too.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

### 4.2 Visualise variables

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("No Standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

After standardisation, become less skewed.

### 4.3 Compute proximity matrix

*dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is *euclidean* proximity matrix. We measure distances between the different cities.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
proxmat
```

### 4.4 Compute hierarchical clustering

We start the hierarchical clustering.

*hclust()* employs agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

We use ward.D.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

We can visualize the tree.

```{r}
plot(hclust_ward, cex = 0.6) #CEX is to scale down resolution to 60% to avoid overlapping
```

### 4.5 Selecting optimal clustering method

agnes()function of cluster package functions like *hclus()*, however, with the *agnes()* function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

We try the different model

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac #output ac which is index value
}

map_dbl(m, ac) #functional programming method - map a list of inputs to a function. Something like looping

#coefficien measures the homogenity
```

Ward has highest coefficient. It is the best. We will use wards from here onwards.

### 4.6 Selecting optimal number of clusters

3 ways

1.  Elbow Method

2.  Average Silhouette Method

3.  Gap Statistic Method

[**GAP Method**]{.underline}

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

```{r}
fviz_gap_stat(gap_stat)
```

Higher the better. 1 cluster does not make sense, same for 2. At least have 3, so we will select 6.

### 4.7 Visualising Dendrograms

Each leaf is one observation. As u move from bottom to top, the most similar leaves will join and eventually the most similar branches will join. The height of the branches represent dissimilarities. Higher, the less similar.

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, #number of clusters
            border = 2:8)
```

## 5. Visually Driven Analysis

We need data to be a data matrix form to create a heatmap.

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

### 5.1 Mapping clusters on the map

```{r}
#Create groups
groups <- as.factor(cutree(hclust_ward, k=6))
```

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.

## 6. Spatially Constrained Clustering - SKATER

We convert shan_df into SpatialPolygonDataFrame because SKATER function only works with sp objects.

```{r}
#Turn into spatial
shan_sp <- as_Spatial(shan_sf)
```

Create neighbor list to create minimum spanning tree.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

```{r}
plot(shan_sp, 
     border=grey(.5))
plot(shan.nb, 
     coordinates(shan_sp), 
     col="blue", 
     add=TRUE)
```

Next, nbcosts() of **spdep** package is used to compute the cost of each edge which is the distance between the node.

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

We convert the neighbour list into a cost matrix.

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

```{r}
shan.mst <- mstree(shan.w)
class(shan.mst)
dim(shan.mst)
```

Note that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.

```{r}
head(shan.mst)
```

```{r}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, 
         coordinates(shan_sp), 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

Number of connections between nodes have been reduced.

### 6.1 Compute spatially constrained clusters using SKATER Method

```{r}
clust6 <- skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

```{r}
str(clust6)
```

```{r}
ccs6 <- clust6$groups
ccs6
```

```{r}
table(ccs6)
```

```{r}
plot(shan_sp, border=gray(.5))
plot(clust6, 
     coordinates(shan_sp), 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

Next we try mapping it on a choropleth map

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`) #make sure order is the same before binding
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

The original one and the one with spatially constrained. Latter is more cohesive.
